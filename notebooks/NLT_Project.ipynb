{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install phonetics\n",
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein\n",
        "!pip install eng-to-ipa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyWijQzg73jU",
        "outputId": "5cb22303-6066-49d9-b67a-11406476ce3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting phonetics\n",
            "  Downloading phonetics-1.0.5.tar.gz (8.8 kB)\n",
            "Building wheels for collected packages: phonetics\n",
            "  Building wheel for phonetics (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phonetics: filename=phonetics-1.0.5-py2.py3-none-any.whl size=8711 sha256=b717734b057c06b1ca0fa8b848e84e68b2a3ed8da44bc9d0a8ef793e838ba430\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/c9/f4/5f43d3212d0aece0feced2484127ddb227ae43d57102aeb259\n",
            "Successfully built phonetics\n",
            "Installing collected packages: phonetics\n",
            "Successfully installed phonetics-1.0.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.20.8-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.20.8\n",
            "  Downloading Levenshtein-0.20.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 52.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.20.8 python-Levenshtein-0.20.8 rapidfuzz-2.13.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting eng-to-ipa\n",
            "  Downloading eng_to_ipa-0.0.2.tar.gz (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 9.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: eng-to-ipa\n",
            "  Building wheel for eng-to-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for eng-to-ipa: filename=eng_to_ipa-0.0.2-py3-none-any.whl size=2822641 sha256=fa90cd0fc7128e5ffa6c64de9af8965db521be02c48a0e6c618cffb6b15a5cb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/c0/dd/aeddfbebc2c3301c3dd09670d9954b0574ac4cd982664c1110\n",
            "Successfully built eng-to-ipa\n",
            "Installing collected packages: eng-to-ipa\n",
            "Successfully installed eng-to-ipa-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZLH_5ZywhjH",
        "outputId": "48586f51-04fb-44cf-9f78-b3db9ab8de89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 33.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.13.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 45.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n",
            "Requirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 45.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers[sentencepiece]) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.9)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.2->transformers[sentencepiece]) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece\n",
            "Successfully installed huggingface-hub-0.10.1 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import random\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=2)"
      ],
      "metadata": {
        "id": "WJwUYWgyskSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_test = \"/content/subtask2-heterographic-test.xml\"\n",
        "path_gold = \"/content/subtask2-heterographic-test.gold\""
      ],
      "metadata": {
        "id": "xjJH6Pf-slfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pun_instances = {}\n",
        "classes = {}\n",
        "locations = {}\n",
        "max_sent_len = None\n",
        "\n",
        "tree = ET.parse(path_test)\n",
        "root = tree.getroot()\n",
        "\n",
        "for child in root:\n",
        "    line = []\n",
        "    idx = child.attrib[\"id\"]\n",
        "    for kid in child:\n",
        "        line.append(kid.text)\n",
        "    pun_instances[idx] = line\n",
        "\n",
        "with open(path_gold) as gold:\n",
        "    lines = gold.readlines()\n",
        "    for line in lines:\n",
        "        token = line.strip().split(\"\\t\")\n",
        "        sub_tokens = token[1].split(\"_\")\n",
        "        locations[token[0]] = sub_tokens[2]\n",
        "\n",
        "all_data = []\n",
        "\n",
        "for idx in pun_instances.keys():\n",
        "    sentence = \" \".join(pun_instances[idx])\n",
        "    pun_word = pun_instances[idx][int(locations[idx]) - 1]\n",
        "    pun_location = int(locations[idx]) - 1    \n",
        "    all_data.append({\"sentence\": sentence, \"location\": pun_location})"
      ],
      "metadata": {
        "id": "gtYFXiayscZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp.pprint(random.sample(all_data, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUY0UPLdtGW6",
        "outputId": "89198815-ce50-4655-b546-e3b7e82025fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ { 'location': 11,\n",
            "    'sentence': \"If you want those ashes , you ' ll have to urn them .\"},\n",
            "  {'location': 6, 'sentence': \"The shy stripper couldn ' t bare it on stage .\"},\n",
            "  { 'location': 14,\n",
            "    'sentence': \"She was only a Quarryman ' s daughter , but she took \"\n",
            "                'everything for granite .'},\n",
            "  {'location': 5, 'sentence': 'A church fair is a bazaar experience .'},\n",
            "  { 'location': 16,\n",
            "    'sentence': 'I was given a shampoo and trim by a Pakistani in Liverpool , '\n",
            "                'said Tom in Urdu .'},\n",
            "  { 'location': 12,\n",
            "    'sentence': 'If an actress has a screaming role , can we say she eeks out '\n",
            "                'a living ?'},\n",
            "  {'location': 5, 'sentence': 'An unemployed logger is a would worker .'},\n",
            "  { 'location': 14,\n",
            "    'sentence': 'Communism is a complicated thing . You must think about from '\n",
            "                'all sorts of Engels .'},\n",
            "  { 'location': 8,\n",
            "    'sentence': 'Another batch of shells for me ! Tom clamored .'},\n",
            "  {'location': 3, 'sentence': \"My name is Gail . I ' m a meteorologist\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identifying Source and Target"
      ],
      "metadata": {
        "id": "tONQpEv4Q9GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from fuzzywuzzy import fuzz\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('cmudict')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHvtXWUlYad0",
        "outputId": "a094a907-7be4-495e-c9e9-7365b5baa7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "O5KouqJLYvig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    pf = \"\".join([i for i in text if i not in string.punctuation])\n",
        "    return pf\n",
        "\n",
        "def turn_to_lowercase(text):\n",
        "  return text.lower()\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "def lemmatizer(text):\n",
        "  lemm_text = \"\".join([wordnet_lemmatizer.lemmatize(word) for word in text])\n",
        "  return lemm_text\n",
        "\n",
        "def tokenization(text):\n",
        "    return text.split(\" \")\n",
        "\n",
        "def remove_stopwords(wordList):\n",
        "    output = [w for w in wordList if w not in stopwords]\n",
        "    return output"
      ],
      "metadata": {
        "id": "6gNo3qIEYziU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"If you want those ashes , you have to urn them .\""
      ],
      "metadata": {
        "id": "1JA4bF7rYiO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_transformed = remove_stopwords(tokenization(lemmatizer(turn_to_lowercase(remove_punctuation(sentence)))))\n",
        "s_transformed = [ x for x in s_transformed if x]\n",
        "s_transformed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_qmqNOvY-dP",
        "outputId": "caf789e2-8cce-4b0b-a2af-2149d5a19c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['want', 'ashes', 'urn']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arpabet = nltk.corpus.cmudict.dict()"
      ],
      "metadata": {
        "id": "StRL0h3raU82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_arp = []\n",
        "for w in s_transformed:\n",
        "  try:\n",
        "    s_arp.append(arpabet[w][0])\n",
        "  except:\n",
        "    s_transformed.remove(w)\n",
        "print(s_arp)\n",
        "print(s_transformed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvuiS41sbdEF",
        "outputId": "7edf6c75-187e-408e-c695-4a34cc2e330a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['W', 'AA1', 'N', 'T'], ['AE1', 'SH', 'AH0', 'Z'], ['ER1', 'N']]\n",
            "['want', 'ashes', 'urn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 90"
      ],
      "metadata": {
        "id": "XYATS4PqZTLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for word in s_arp:\n",
        "  itemList = []\n",
        "  for w, a in arpabet.items():\n",
        "    if fuzz.ratio(word, a) > threshold and w not in s_transformed:\n",
        "      itemList.append(w)\n",
        "  results.append(itemList)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82YstihPbJY3",
        "outputId": "aaf3de45-7018-4ddb-9272-854c35376a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['bondt',\n",
              "  'bonte',\n",
              "  'font',\n",
              "  'fonte',\n",
              "  'mont',\n",
              "  'pont',\n",
              "  'ponte',\n",
              "  'waft',\n",
              "  'wand',\n",
              "  'wonk'],\n",
              " ['ashen', 'asses'],\n",
              " ['earn', 'erne']]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WordNet Senses"
      ],
      "metadata": {
        "id": "D7mkEdxW8Bel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "import nltk\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iuZOv8C8XPi",
        "outputId": "d1b8b1c3-6d40-424b-f708-8de010d21c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"If you want those ashes , you have to urn them .\""
      ],
      "metadata": {
        "id": "3RmDqHTR-jVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source = \"urn\"\n",
        "target = \"earn\""
      ],
      "metadata": {
        "id": "FXdDh-0B8BBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function to map\n",
        "s_mapping = wn.NOUN\n",
        "t_mapping = wn.VERB"
      ],
      "metadata": {
        "id": "a_7juWilAI63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_pos = None\n",
        "sent_pos = pos_tag(sent.split())\n",
        "for w, pos in sent_pos:\n",
        "  if w == source:\n",
        "    res_pos = pos[0].lower()\n",
        "res_pos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "ExxIEkiH-ovY",
        "outputId": "c568adde-8b27-4c59-be20-02175a1377dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'v'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_set = wn.synsets(source, pos=s_mapping)\n",
        "tgt_set = wn.synsets(target, pos=t_mapping)\n",
        "print(src_set[0].definition())\n",
        "print(tgt_set[0].definition())\n",
        "print(src_set[0].path_similarity(tgt_set[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwuZUlBV8QS4",
        "outputId": "1a55e6d4-801c-43d3-da78-25aac045e24e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a large vase that usually has a pedestal or feet\n",
            "earn on some commercial or business transaction; earn as salary or wages\n",
            "0.07142857142857142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synsets('knows')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlZAc4bTyHs2",
        "outputId": "4ad57dfc-bc90-4134-8204-9ffe779afae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('know.n.01'),\n",
              " Synset('know.v.01'),\n",
              " Synset('know.v.02'),\n",
              " Synset('know.v.03'),\n",
              " Synset('know.v.04'),\n",
              " Synset('know.v.05'),\n",
              " Synset('acknowledge.v.06'),\n",
              " Synset('know.v.07'),\n",
              " Synset('sleep_together.v.01'),\n",
              " Synset('know.v.09'),\n",
              " Synset('know.v.10'),\n",
              " Synset('know.v.11')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sound Similarity"
      ],
      "metadata": {
        "id": "LMa2T1YCK41T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3natn03Goas",
        "outputId": "a35ce84e-5a67-411f-8e4a-88fa21d2abab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: phonetics in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.7/dist-packages (0.18.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (0.20.8)\n",
            "Requirement already satisfied: Levenshtein==0.20.8 in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (0.20.8)\n",
            "Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from Levenshtein==0.20.8->python-Levenshtein) (2.13.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting eng-to-ipa\n",
            "  Downloading eng_to_ipa-0.0.2.tar.gz (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 4.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: eng-to-ipa\n",
            "  Building wheel for eng-to-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for eng-to-ipa: filename=eng_to_ipa-0.0.2-py3-none-any.whl size=2822641 sha256=22d7e2bc6f257ba57f7a85395222ac149052db9219d9cfcd6a259e23731b2642\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/c0/dd/aeddfbebc2c3301c3dd09670d9954b0574ac4cd982664c1110\n",
            "Successfully built eng-to-ipa\n",
            "Installing collected packages: eng-to-ipa\n",
            "Successfully installed eng-to-ipa-0.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install phonetics\n",
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein\n",
        "!pip install eng-to-ipa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "import phonetics\n",
        "import eng_to_ipa as ipa\n",
        "import nltk"
      ],
      "metadata": {
        "id": "bWjqSSwWH8Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Double Metaphone"
      ],
      "metadata": {
        "id": "4MsVPQ31KcGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_1_dmeta = phonetics.dmetaphone('propane')\n",
        "print(word_1_dmeta)\n",
        "word_2_dmeta = phonetics.dmetaphone('profane')\n",
        "print(word_2_dmeta)\n",
        "print(fuzz.ratio(word_1_dmeta, word_2_dmeta))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjxDue0lIdb-",
        "outputId": "0da311e9-c926-4a0e-b6e7-b0a339248f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('PRPN', '')\n",
            "('PRFN', '')\n",
            "92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IPA"
      ],
      "metadata": {
        "id": "pcCdAGiRKfcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_1_ipa = ipa.convert('propane')\n",
        "print(word_1_ipa)\n",
        "word_2_ipa = ipa.convert('profane')\n",
        "print(word_2_ipa)\n",
        "print(fuzz.ratio(word_1_ipa, word_2_ipa))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-p9bdosKg21",
        "outputId": "169a7892-8eee-4b38-d9ce-94f0e75bf98a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ˈproʊˌpeɪn\n",
            "proʊˈfeɪn\n",
            "74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ARPABET"
      ],
      "metadata": {
        "id": "Sm-iNUBkXxcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arpabet = nltk.corpus.cmudict.dict()\n",
        "word_1_arp = arpabet['propane']\n",
        "print(word_1_ipa)\n",
        "word_2_arp = arpabet['profane']\n",
        "print(word_2_ipa)\n",
        "print(fuzz.ratio(word_1_arp, word_2_arp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh8nxmSfXy7w",
        "outputId": "2efeb006-0871-47b2-a37a-4d2f0f71a9f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ˈproʊˌpeɪn\n",
            "proʊˈfeɪn\n",
            "92\n"
          ]
        }
      ]
    }
  ]
}